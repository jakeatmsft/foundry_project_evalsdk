{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Evaluation – sample notebook\n",
    "\n",
    "This notebook mirrors the functionality of the original Python script that demonstrates many evaluators available in the `azure-ai-evaluation` package.\n",
    "\n",
    "⚠️ **Before running**: set these environment variables (e.g. in a `.env` file or in the shell that launches Jupyter).\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_ENDPOINT   = https://<account_name>.services.ai.azure.com\n",
    "AZURE_OPENAI_KEY        = <your-key>\n",
    "AZURE_OPENAI_DEPLOYMENT = <deployment-name>\n",
    "AZURE_OPENAI_API_VERSION= 2024-02-15-preview     # or the version you use\n",
    "\n",
    "# For Azure AI Content Safety (optional – only needed for those cells):\n",
    "AZURE_AI_PROJECT_URL    = https://<resource>.services.ai.azure.com/api/projects/<project>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa715b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, pandas as pd\n",
    "\n",
    "def save_evaluation_results(evaluation, output_path=\"evaluation_results.jsonl\"):\n",
    "    \"\"\"\n",
    "    Save evaluation results to a JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : dict or object with to_pandas() method\n",
    "        The evaluation results to save\n",
    "    output_path : str or pathlib.Path, optional\n",
    "        Path to save the JSONL file (default: \"evaluation_results.jsonl\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of rows saved\n",
    "    \"\"\"\n",
    "    # If `evaluation` is a dict (as shown) grab the rows list;\n",
    "    # otherwise, call `evaluation.to_pandas()` to convert first.\n",
    "    rows = evaluation[\"rows\"] if isinstance(evaluation, dict) else evaluation.to_pandas().to_dict(\"records\")\n",
    "    \n",
    "    # ── JSONL (preferred for re-loading programmatically) ────────────\n",
    "    jsonl_path = pathlib.Path(output_path)\n",
    "    with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(rows)} rows → {jsonl_path}\")\n",
    "    return len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'azure_endpoint': 'https://ai-1965.openai.azure.com/',\n",
       " 'api_key': '',\n",
       " 'azure_deployment': 'gpt-4.1-mini'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"\"Utility cell — load .env (if present) and build a common `model_config`.\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads variables from a .env file in the same directory (optional)\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\":   os.getenv(\"AZURE_OPENAI_ENDPOINT\"),    # https://<account>.services.ai.azure.com\n",
    "    \"api_key\":          os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    # If your SDK version needs it, add \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Batch evaluation with `evaluate(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-10 13:48:15 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-10 13:48:15 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-10 13:48:15 -0700][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-10 13:48:15 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_coherence_20250610_134815_069318, log path: C:\\Users\\jacwang\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_coherence_20250610_134815_069318\\logs.txt\n",
      "[2025-06-10 13:48:15 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250610_134815_071989, log path: C:\\Users\\jacwang\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_relevance_20250610_134815_071989\\logs.txt\n",
      "[2025-06-10 13:48:15 -0700][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_intent_resolution_20250610_134815_073102, log path: C:\\Users\\jacwang\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_intent_resolution_20250610_134815_073102\\logs.txt\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    evaluate,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    ResponseCompletenessEvaluator,\n",
    ")\n",
    "\n",
    "data_path = \"../../data/evaluate_test_data.jsonl\"  # adjust if necessary\n",
    "\n",
    "eval_results = evaluate(\n",
    "    data=data_path,\n",
    "    evaluators={\n",
    "        \"coherence\":          CoherenceEvaluator(model_config=model_config),\n",
    "        \"relevance\":          RelevanceEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\":  IntentResolutionEvaluator(model_config=model_config),\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"coherence\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"query\":    \"${data.query}\",\n",
    "            },\n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\":  \"${data.context}\",\n",
    "                \"query\":    \"${data.query}\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fffcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_filename = \"evaluation_results.jsonl\"\n",
    "# Save the evaluation results to a JSONL file\n",
    "save_evaluation_results(eval_results, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2  Individual evaluators\n",
    "\n",
    "Each of the following cells shows a self-contained example.  Execute only what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bleu\n",
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu = BleuScoreEvaluator()\n",
    "bleu(response=\"Lyon is the capital of France.\", ground_truth=\"Paris is the capital of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence\n",
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coh = CoherenceEvaluator(model_config=model_config)\n",
    "coh(query=\"What is the capital of France?\", response=\"Paris is the capital of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Resolution\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator\n",
    "\n",
    "intent = IntentResolutionEvaluator(model_config=model_config)\n",
    "intent(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Opening hours of the Eiffel Tower are 9:00 AM – 11:00 PM.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Content-safety-based evaluators (Azure AI Content Safety)\n",
    "These require `AZURE_AI_PROJECT_URL` and an Azure credential (e.g. `DefaultAzureCredential`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    SexualEvaluator,\n",
    "    ViolenceEvaluator,\n",
    ")\n",
    "\n",
    "azure_ai_project = os.getenv(\"AZURE_AI_PROJECT_URL\")\n",
    "cred = DefaultAzureCredential()\n",
    "\n",
    "content_safe = ContentSafetyEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "content_safe(query=\"Who are you?\", response=\"I am an AI assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional safety evaluators – run as needed\n",
    "hate      = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "selfharm  = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "sexual    = SexualEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "violence  = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "\n",
    "hate(query=\"Where are you from?\", response=\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1 = F1ScoreEvaluator()\n",
    "f1(response=\"Lyon is the capital of France.\", ground_truth=\"Paris is the capital of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluency\n",
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency = FluencyEvaluator(model_config=model_config)\n",
    "fluency(response=\"Paris is the capital of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLEU\n",
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "gleu = GleuScoreEvaluator()\n",
    "gleu(response=\"Paris is the capital of France.\", ground_truth=\"France's capital is Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundedness\n",
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "ground = GroundednessEvaluator(model_config=model_config)\n",
    "ground(\n",
    "    response=\"Paris is the capital of France.\",\n",
    "    context=(\n",
    "        \"France, located in Western Europe, … \"\n",
    "        \"Paris is renowned for art, fashion, the Eiffel Tower, and the Louvre.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteor\n",
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor = MeteorScoreEvaluator(alpha=0.8)\n",
    "meteor(response=\"Paris is the capital of France.\", ground_truth=\"France's capital is Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protected Material\n",
    "from azure.ai.evaluation import ProtectedMaterialEvaluator\n",
    "\n",
    "prot = ProtectedMaterialEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "prot(\n",
    "    query=\"Write me a catchy song\",\n",
    "    response=(\n",
    "        \"You are the dancing queen, young and sweet, only seventeen. \"\n",
    "        \"Feel the beat from the tambourine, oh yeah.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Evaluator\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa = QAEvaluator(model_config=model_config)\n",
    "qa(query=\"This's the color?\", response=\"Black\", ground_truth=\"gray\", context=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "rel = RelevanceEvaluator(model_config=model_config)\n",
    "rel(query=\"What is the capital of Japan?\", response=\"The capital of Japan is Tokyo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval\n",
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "\n",
    "ret = RetrievalEvaluator(model_config=model_config)\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\"content\": \"What is the capital of France?\", \"role\": \"user\", \"context\": \"Customer wants to know the capital of France\"},\n",
    "        {\"content\": \"Paris\", \"role\": \"assistant\", \"context\": \"Paris is the capital of France\"},\n",
    "        {\"content\": \"What is the capital of Hawaii?\", \"role\": \"user\", \"context\": \"Customer wants to know the capital of Hawaii\"},\n",
    "        {\"content\": \"Honolulu\", \"role\": \"assistant\", \"context\": \"Honolulu is the capital of Hawaii\"}\n",
    "    ],\n",
    "    \"context\": \"Global context\"\n",
    "}\n",
    "ret(conversation=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-4\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_4)\n",
    "rouge(response=\"Paris is the capital of France.\", ground_truth=\"France's capital is Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity\n",
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "sim = SimilarityEvaluator(model_config=model_config)\n",
    "sim(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\",\n",
    "    ground_truth=\"Tokyo is Japan's capital.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Completeness\n",
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "comp = ResponseCompletenessEvaluator(model_config=model_config)\n",
    "comp(response=\"The capital of Japan is Tokyo.\", ground_truth=\"Tokyo is Japan's capital.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Adherence\n",
    "from azure.ai.evaluation import TaskAdherenceEvaluator\n",
    "\n",
    "task = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "query_msg = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful customer service agent.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is the status of my order #123?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response_msg = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call\": {\n",
    "                    \"id\": \"tool_001\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": \"get_order\", \"arguments\": {\"order_id\": \"123\"}},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": \"tool_001\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_result\": \"{ \\\"order\\\": { \\\"id\\\": \\\"123\\\", \\\"status\\\": \\\"shipped\\\" } }\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Your order #123 has been shipped.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_defs = [\n",
    "    {\n",
    "        \"name\": \"get_order\",\n",
    "        \"description\": \"Get order details.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"order_id\": {\"type\": \"string\"}},\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "task(query=query_msg, response=response_msg, tool_definitions=tool_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Pro evaluators (Groundedness Pro, Indirect Attack …)\n",
    "Also require `AZURE_AI_PROJECT_URL` + credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import IndirectAttackEvaluator, GroundednessProEvaluator\n",
    "\n",
    "indirect = IndirectAttackEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "ground_pro = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=cred)\n",
    "\n",
    "indirect(query=\"What is the capital of France?\", response=\"Paris\")\n",
    "ground_pro(\n",
    "    query=\"What shape has 4 equilateral sides?\",\n",
    "    response=\"Rhombus\",\n",
    "    context=\"Rhombus is a shape with 4 equilateral sides.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done\n",
    "You can now explore or adapt the individual evaluator calls as required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
