{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloud evaluation: Evaluating AI app data remotely in the cloud \n",
    "\n",
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to evaluate data generated by AI applications or LLMs remotely in the cloud. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [Azure AI Safety Evaluation](https://aka.ms/azureaistudiosafetyeval)\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 20 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates the cloud evaluation of query and response pairs that were generated by an AI app or a LLM. It is important to have access to AzureOpenAI credentials and an AzureAI project. **To create data to use in your own evaluation, learn more [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data)** . This example demonstrates: \n",
    "\n",
    "- Single-instance, triggered cloud evaluation on a test dataset (to be used for pre-deployment evaluation of an AI application).\n",
    "\n",
    "## Before you begin\n",
    "### Prerequesite\n",
    "- Have an Azure OpenAI Deployment with GPT model supporting `chat completion`, for example `gpt-4`.\n",
    "- Make sure you're first logged into your Azure subscription by running `az login`.\n",
    "- You have some test data you want to evaluate, which includes the user queries and responses (and perhaps context, or ground truth) from your AI applications. See [data requirements for our built-in evaluators](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#data-requirements-for-built-in-evaluators). Alternatively, if you want to simulate data against your application endpoints using Azure AI Evaluation SDK, see our samples on simulation. \n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login --tenant [tenant_id (directory_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_connection_string = os.environ.get('AZURE_AI_PROJECT_URL')  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    "azure_openai_deployment = os.environ.get('AZURE_OPENAI_DEPLOYMENT')  # Your AOAI resource, you must use an AOAI GPT model\n",
    "azure_openai_api_version = os.environ.get('AZURE_OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional â€“ reuse an existing dataset\n",
    "dataset_name    = os.environ.get(\"DATASET_NAME\",    \"dataset-test\")\n",
    "dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to your Azure Open AI deployment\n",
    "To evaluate your LLM-generated data remotely in the cloud, we must connect to your Azure Open AI deployment. This deployment must be a GPT model which supports `chat completion`, such as `gpt-4`. To see the proper value for `conn_str`, navigate to the connection string at the \"Project Overview\" page for your Azure AI project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the project client (Foundry project and credentials)\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=azure_ai_connection_string,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use `evaluate_test_data.jsonl` which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the [`Simulator`](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data) from Azure AI Evaluation SDK. \n",
    "\n",
    "Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your [registry](https://ml.azure.com/registries) or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List each dataset with name, version and id\n",
    "for ds in project_client.datasets.list():\n",
    "    print(f\"- {ds.name}  (version: {ds.version}, id: {ds.id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a local jsonl file (skip if you already have a Dataset registered)\n",
    "try:\n",
    "    data_id = project_client.datasets.upload_file(\n",
    "        name='eval-data',\n",
    "        version=dataset_version,\n",
    "        file_path=\"../../data/evaluate_test_data.jsonl\",\n",
    "    ).id\n",
    "    print(f\"Successfully uploaded dataset with ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    if \"409\" in str(e) or \"Conflict\" in str(e):\n",
    "        print(f\"Dataset 'eval-data' with version '{dataset_version}' already exists.\")\n",
    "        # Retrieve the existing dataset\n",
    "        datasets = project_client.datasets.list()\n",
    "        for ds in datasets:\n",
    "            if ds.name == 'eval-data' and ds.version == dataset_version:\n",
    "                data_id = ds.id\n",
    "                print(f\"Using existing dataset with ID: {data_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "The code below demonstrates how to configure the evaluators you want to run in AI Foundry.  Note, the EvaluatorConfiguration must be configured correctly for the entire cloud evaluation job to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working evals, use to test in support region like East US2\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"hate_unfairness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.HATE_UNFAIRNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value,\n",
    "    ),\n",
    "    \"code_vulnerability\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CODE_VULNERABILITY.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.F1_SCORE.value,\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value,\n",
    "          init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"gleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GLEU_SCORE.value,\n",
    "    ),\n",
    "    \"indirect_attack\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INDIRECT_ATTACK.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"meteor_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.METEOR_SCORE.value,\n",
    "        init_params={\"alpha\": 0.8},\n",
    "    ),\n",
    "    \"protected_material\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.PROTECTED_MATERIAL.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RETRIEVAL.value,\n",
    "         init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            },\n",
    "    ),\n",
    "    \"rouge_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.ROUGE_SCORE.value,\n",
    "        init_params={\"rouge_type\": RougeType.ROUGE_4},\n",
    "    ),\n",
    "    \"self_harm\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SELF_HARM.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"sexual\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SEXUAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"text_similarity\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TEXT_SIMILARITY_GRADER.value,\n",
    "        init_params={\n",
    "            \"evaluation_metric\": \"fuzzy_match\",\n",
    "            \"input\": \"{{item.response}}\",\n",
    "            \"name\": \"similarity\",\n",
    "            \"pass_threshold\": 1,\n",
    "            \"reference\": \"{{item.ground_truth}}\",\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            },\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud evaluation\n",
    "Below we demonstrate how to trigger each Cloud Evaluation remotely on a test dataset.  The examples runs each evaluation individually, you can also execute them in a single evaluation. This can be used for pre-deployment testing of your AI application. \n",
    " \n",
    "Here we pass in the `data_id` we would like to use for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each evaluator individually using the same dataset\n",
    "from azure.ai.projects.models import Evaluation, InputDataset\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "for eval_name, eval_config in evaluators.items():\n",
    "    print(f\"\\nRunning evaluation for: {eval_name}\")\n",
    "    single_eval = Evaluation(\n",
    "        display_name=f\"Cloud evaluation - {eval_name}\",\n",
    "        description=f\"Evaluation of dataset with {eval_name}\",\n",
    "        data=InputDataset(id=data_id),\n",
    "        evaluators={eval_name: eval_config},\n",
    "    )\n",
    "    try:\n",
    "        eval_response = project_client.evaluations.create(\n",
    "            single_eval,\n",
    "            headers={\n",
    "                \"model-endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                \"api-key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "            },\n",
    "        )\n",
    "        print(f\"Created evaluation: {eval_response.name}\")\n",
    "        print(f\"Status: {eval_response.status}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running evaluator {eval_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = eval_response\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "#print(\"Created evaluation, evaluation ID: \", get_evaluation_response.data.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
