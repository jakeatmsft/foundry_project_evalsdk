{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloud evaluation: Evaluating AI app data remotely in the cloud \n",
    "\n",
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to evaluate data generated by AI applications or LLMs remotely in the cloud. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [Azure AI Safety Evaluation](https://aka.ms/azureaistudiosafetyeval)\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 20 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates the cloud evaluation of query and response pairs that were generated by an AI app or a LLM. It is important to have access to AzureOpenAI credentials and an AzureAI project. **To create data to use in your own evaluation, learn more [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data)** . This example demonstrates: \n",
    "\n",
    "- Single-instance, triggered cloud evaluation on a test dataset (to be used for pre-deployment evaluation of an AI application).\n",
    "\n",
    "## Before you begin\n",
    "### Prerequesite\n",
    "- Have an Azure OpenAI Deployment with GPT model supporting `chat completion`, for example `gpt-4`.\n",
    "- Make sure you're first logged into your Azure subscription by running `az login`.\n",
    "- You have some test data you want to evaluate, which includes the user queries and responses (and perhaps context, or ground truth) from your AI applications. See [data requirements for our built-in evaluators](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#data-requirements-for-built-in-evaluators). Alternatively, if you want to simulate data against your application endpoints using Azure AI Evaluation SDK, see our samples on simulation. \n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"id\": \"6025ba02-1dfd-407f-b358-88f811c7c7aa\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [\n",
      "      {\n",
      "        \"tenantId\": \"2f4a9838-26b7-47ee-be60-ccc1fdec5953\"\n",
      "      },\n",
      "      {\n",
      "        \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
      "      }\n",
      "    ],\n",
      "    \"name\": \"MCAPS-Hybrid-REQ-40165-2022-JakeWang\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"user\": {\n",
      "      \"name\": \"jacwang@microsoft.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"id\": \"997499f7-6523-407d-ac0c-d9ee154f1df1\",\n",
      "    \"isDefault\": false,\n",
      "    \"managedByTenants\": [\n",
      "      {\n",
      "        \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
      "      }\n",
      "    ],\n",
      "    \"name\": \"MCAPS-Hybrid-REQ-41592-2022-KamalAbburi\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"user\": {\n",
      "      \"name\": \"jacwang@microsoft.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Select the account you want to log in with. For more information on login with Azure CLI, see https://go.microsoft.com/fwlink/?linkid=2271136\n"
     ]
    }
   ],
   "source": [
    "!az login --tenant 16b3c013-d300-468d-ac64-7eda0820b6d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_connection_string = os.environ.get('AZURE_AI_PROJECT_URL')  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    "azure_openai_deployment = os.environ.get('AZURE_OPENAI_DEPLOYMENT')  # Your AOAI resource, you must use an AOAI GPT model\n",
    "azure_openai_api_version = os.environ.get('AZURE_OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional â€“ reuse an existing dataset\n",
    "dataset_name    = os.environ.get(\"DATASET_NAME\",    \"dataset-test\")\n",
    "dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to your Azure Open AI deployment\n",
    "To evaluate your LLM-generated data remotely in the cloud, we must connect to your Azure Open AI deployment. This deployment must be a GPT model which supports `chat completion`, such as `gpt-4`. To see the proper value for `conn_str`, navigate to the connection string at the \"Project Overview\" page for your Azure AI project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the project client (Foundry project and credentials)\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=azure_ai_connection_string,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use `evaluate_test_data.jsonl` which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the [`Simulator`](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data) from Azure AI Evaluation SDK. \n",
    "\n",
    "Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your [registry](https://ml.azure.com/registries) or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nw_dataset  (version: 1.0, id: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/nw_dataset/versions/1.0)\n",
      "- eval-data  (version: 1.0, id: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/eval-data/versions/1.0)\n"
     ]
    }
   ],
   "source": [
    "# List each dataset with name, version and id\n",
    "for ds in project_client.datasets.list():\n",
    "    print(f\"- {ds.name}  (version: {ds.version}, id: {ds.id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'eval-data' with version '1.0' already exists.\n",
      "Using existing dataset with ID: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/eval-data/versions/1.0\n"
     ]
    }
   ],
   "source": [
    "# Upload a local jsonl file (skip if you already have a Dataset registered)\n",
    "try:\n",
    "    data_id = project_client.datasets.upload_file(\n",
    "        name='eval-data',\n",
    "        version=dataset_version,\n",
    "        file_path=\"../../data/evaluate_test_data.jsonl\",\n",
    "    ).id\n",
    "    print(f\"Successfully uploaded dataset with ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    if \"409\" in str(e) or \"Conflict\" in str(e):\n",
    "        print(f\"Dataset 'eval-data' with version '{dataset_version}' already exists.\")\n",
    "        # Retrieve the existing dataset\n",
    "        datasets = project_client.datasets.list()\n",
    "        for ds in datasets:\n",
    "            if ds.name == 'eval-data' and ds.version == dataset_version:\n",
    "                data_id = ds.id\n",
    "                print(f\"Using existing dataset with ID: {data_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "The code below demonstrates how to configure the evaluators you want to run in AI Foundry.  Note, the EvaluatorConfiguration must be configured correctly for the entire cloud evaluation job to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_AI_PROJECT_URL\"), # https://<account_name>.services.ai.azure.com\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import EvaluatorConfiguration, EvaluatorIds\n",
    "from azure.ai.evaluation import RougeType\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"response_completeness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RESPONSE_COMPLETENESS.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"hate_unfairness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.HATE_UNFAIRNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"content_safety\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CONTENT_SAFETY.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"self_harm\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SELF_HARM.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"sexual\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SEXUAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"protected_material\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.PROTECTED_MATERIAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value\n",
    "    ),\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.F1_SCORE.value\n",
    "    ),\n",
    "    \"gleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GLEU_SCORE.value\n",
    "    ),\n",
    "    \"meteor_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.METEOR_SCORE.value,\n",
    "        init_params={\"alpha\": 0.8},\n",
    "    ),\n",
    "    \"rouge_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.ROUGE_SCORE.value,\n",
    "        init_params={\"rouge_type\": RougeType.ROUGE_4},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"groundedness_pro\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS_PRO.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"code_vulnerability\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CODE_VULNERABILITY.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value\n",
    "    ),\n",
    "    \"indirect_attack\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INDIRECT_ATTACK.value\n",
    "    ),\n",
    "    \"retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RETRIEVAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"similarity\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SIMILARITY.value\n",
    "    ),\n",
    "    \"document_retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.DOCUMENT_RETRIEVAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"tool_call_accuracy\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TOOL_CALL_ACCURACY.value\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working evals, use to test in support region like East US2\n",
    "'''from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    # \"relevance\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.RELEVANCE.value,\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     #data_mapping={\"query\": \"${data.Input}\", \"response\": \"${data.Output}\"},\n",
    "    # ),\n",
    "    \"hate_unfairness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.HATE_UNFAIRNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    # \"groundedness\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.GROUNDEDNESS.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    # \"groundedness_pro\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.GROUNDEDNESS_PRO.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value,\n",
    "        # init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    # \"code_vulnerability\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.CODE_VULNERABILITY.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    # \"coherence\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.COHERENCE.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    # \"content_safety\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.CONTENT_SAFETY.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.F1_SCORE.value,\n",
    "        # init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    # \"fluency\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.FLUENCY.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    \"gleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GLEU_SCORE.value,\n",
    "        # init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    # \"indirect_attack\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.INDIRECT_ATTACK.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    # \"intent_resolution\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    \"meteor_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.METEOR_SCORE.value,\n",
    "        init_params={\"alpha\": 0.8},\n",
    "    ),\n",
    "    # \"protected_material\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.PROTECTED_MATERIAL.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    # \"retrieval\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.RETRIEVAL.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "    \"rouge_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.ROUGE_SCORE.value,\n",
    "        init_params={\"rouge_type\"=RougeType.ROUGE_4},\n",
    "    ),\n",
    "    \"self_harm\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SELF_HARM.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"sexual\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SEXUAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    # \"similarity\": EvaluatorConfiguration(\n",
    "    #     id=EvaluatorIds.SIMILARITY.value,\n",
    "    #     init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    # ),\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud evaluation\n",
    "Below we demonstrate how to trigger a single-instance Cloud Evaluation remotely on a test dataset. This can be used for pre-deployment testing of your AI application. \n",
    " \n",
    "Here we pass in the `data_id` we would like to use for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) Invalid connectionId format: 335nuZ4bUIaJgL6iyOjJN4Ggxk5jxX5v2GXGlYWHrMt11OZQWr04JQQJ99BEACHYHv6XJ3w3AAAAACOGYS8f. Expected 'connections/{name}' or 'connections/{name}/...' is invalid\nCode: UserError\nMessage: Invalid connectionId format: 335nuZ4bUIaJgL6iyOjJN4Ggxk5jxX5v2GXGlYWHrMt11OZQWr04JQQJ99BEACHYHv6XJ3w3AAAAACOGYS8f. Expected 'connections/{name}' or 'connections/{name}/...' is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m Evaluation(\n\u001b[0;32m      8\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCloud evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation of dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     data\u001b[38;5;241m=\u001b[39mInputDataset(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mdata_id),\n\u001b[0;32m     11\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39mevaluators,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run the evaluation \u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m evaluation_response \u001b[38;5;241m=\u001b[39m \u001b[43mproject_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi-key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation_response\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation_response\u001b[38;5;241m.\u001b[39mstatus)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\ai\\projects\\_validation.py:46\u001b[0m, in \u001b[0;36mapi_version_validation.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsupported:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     39\u001b[0m             [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m         )\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\ai\\projects\\operations\\_operations.py:1151\u001b[0m, in \u001b[0;36mEvaluationsOperations.create\u001b[1;34m(self, evaluation, **kwargs)\u001b[0m\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[0;32m   1154\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_bytes()\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (UserError) Invalid connectionId format: 335nuZ4bUIaJgL6iyOjJN4Ggxk5jxX5v2GXGlYWHrMt11OZQWr04JQQJ99BEACHYHv6XJ3w3AAAAACOGYS8f. Expected 'connections/{name}' or 'connections/{name}/...' is invalid\nCode: UserError\nMessage: Invalid connectionId format: 335nuZ4bUIaJgL6iyOjJN4Ggxk5jxX5v2GXGlYWHrMt11OZQWr04JQQJ99BEACHYHv6XJ3w3AAAAACOGYS8f. Expected 'connections/{name}' or 'connections/{name}/...' is invalid"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "\n",
    "# Create an evaluation with the dataset and evaluators specified\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=InputDataset(id=data_id),\n",
    "    evaluators=evaluators,\n",
    ")\n",
    "\n",
    "# Run the evaluation \n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"api-key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Created evaluation:\", evaluation_response.name)\n",
    "print(\"Status:\", evaluation_response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'azureai://accounts/ai-jacwang-1965/projects/evalproj/data/eval-data/versions/1.0', 'type': 'Dataset'}, 'target': None, 'description': 'Evaluation of dataset', 'evaluators': {'hate_unfairness': {'id': 'azureai://built-in/evaluators/hate_unfairness', 'initParams': {'azure_ai_project': 'https://ai-jacwang-1965.services.ai.azure.com/api/projects/evalproj'}, 'dataMapping': {}}, 'violence': {'id': 'azureai://built-in/evaluators/violence', 'initParams': {'azure_ai_project': 'https://ai-jacwang-1965.services.ai.azure.com/api/projects/evalproj'}, 'dataMapping': {}}, 'bleu_score': {'id': 'azureai://built-in/evaluators/bleu_score', 'initParams': {}, 'dataMapping': {}}, 'f1_score': {'id': 'azureai://built-in/evaluators/f1_score', 'initParams': {}, 'dataMapping': {}}, 'gleu_score': {'id': 'azureai://built-in/evaluators/gleu_score', 'initParams': {}, 'dataMapping': {}}, 'meteor_score': {'id': 'azureai://built-in/evaluators/meteor_score', 'initParams': {'alpha': 0.8}, 'dataMapping': {}}, 'self_harm': {'id': 'azureai://built-in/evaluators/self_harm', 'initParams': {'azure_ai_project': 'https://ai-jacwang-1965.services.ai.azure.com/api/projects/evalproj'}, 'dataMapping': {}}, 'sexual': {'id': 'azureai://built-in/evaluators/sexual', 'initParams': {'azure_ai_project': 'https://ai-jacwang-1965.services.ai.azure.com/api/projects/evalproj'}, 'dataMapping': {}}}, 'id': 'cd97d194-f465-45ba-9b67-2c97fe524ad6', 'displayName': 'Cloud evaluation', 'tags': {}, 'properties': {'runType': 'eval_run', '_azureml.evaluation_run': 'evaluation.service', '_azureml.evaluate_artifacts': '[{\"path\": \"instance_results.jsonl\", \"type\": \"table\"}]', 'AiStudioEvaluationUri': 'https://ai.azure.com/resource/build/evaluation/cd97d194-f465-45ba-9b67-2c97fe524ad6?wsid=/subscriptions/6025ba02-1dfd-407f-b358-88f811c7c7aa/resourceGroups/aigent_eus/providers/Microsoft.CognitiveServices/accounts/ai-jacwang-1965/projects/evalproj&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', '_azureml.evaluation_name_map_length': '1', '_azureml.evaluation_name_map_0': '{\"hate_unfairness\":\"hate_unfairness\",\"violence\":\"violence\",\"bleu_score\":\"bleu_score\",\"f1_score\":\"f1_score\",\"gleu_score\":\"gleu_score\",\"meteor_score\":\"meteor_score\",\"self_harm\":\"self_harm\",\"sexual\":\"sexual\"}'}, 'status': 'NotStarted', 'outputs': {}, 'systemData': {'createdAt': '06/13/2025 18:52:07 +00:00', 'createdBy': 'Jake Wang', 'modifiedBy': 'Jake Wang'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Evaluation status:  NotStarted\n",
      "AI Foundry Portal URI:  https://ai.azure.com/resource/build/evaluation/8615bd26-9b45-4abc-8973-0475773ded38?wsid=/subscriptions/6025ba02-1dfd-407f-b358-88f811c7c7aa/resourceGroups/aigent_eus/providers/Microsoft.CognitiveServices/accounts/ai-jacwang-1965/projects/evalproj&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = evaluation_response\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "#print(\"Created evaluation, evaluation ID: \", get_evaluation_response.data.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and download evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from azure.ai.projects.models import Evaluation\n",
    "\n",
    "# Poll for evaluation completion\n",
    "eval_name = evaluation_response.name\n",
    "while True:\n",
    "    ev = project_client.evaluations.get(eval_name)\n",
    "    status = ev.status\n",
    "    print(f\"Evaluation status: {status}\")\n",
    "    if status in ['Completed', 'Failed', 'Canceled']:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "# List evaluation result versions for this evaluation\n",
    "print('Evaluation result versions:')\n",
    "for res in project_client.evaluation_results.list_versions(eval_name):\n",
    "    print(f\"- version: {res.version}, blob URI: {res.blob_uri}\")\n",
    "    result = res\n",
    "    break\n",
    "\n",
    "# Download the instance results JSONL\n",
    "import requests\n",
    "blob_url = result.blob_uri\n",
    "r = requests.get(blob_url)\n",
    "with open('instance_results.jsonl', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "print('Downloaded instance_results.jsonl')\n",
    "\n",
    "# Preview the first few rows\n",
    "with open('instance_results.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
