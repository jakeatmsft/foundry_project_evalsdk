{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloud evaluation: Evaluating AI app data remotely in the cloud \n",
    "\n",
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to evaluate data generated by AI applications or LLMs remotely in the cloud. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [Azure AI Safety Evaluation](https://aka.ms/azureaistudiosafetyeval)\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 20 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates the cloud evaluation of query and response pairs that were generated by an AI app or a LLM. It is important to have access to AzureOpenAI credentials and an AzureAI project. **To create data to use in your own evaluation, learn more [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data)** . This example demonstrates: \n",
    "\n",
    "- Single-instance, triggered cloud evaluation on a test dataset (to be used for pre-deployment evaluation of an AI application).\n",
    "\n",
    "## Before you begin\n",
    "### Prerequesite\n",
    "- Have an Azure OpenAI Deployment with GPT model supporting `chat completion`, for example `gpt-4`.\n",
    "- Make sure you're first logged into your Azure subscription by running `az login`.\n",
    "- You have some test data you want to evaluate, which includes the user queries and responses (and perhaps context, or ground truth) from your AI applications. See [data requirements for our built-in evaluators](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#data-requirements-for-built-in-evaluators). Alternatively, if you want to simulate data against your application endpoints using Azure AI Evaluation SDK, see our samples on simulation. \n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"id\": \"6025ba02-1dfd-407f-b358-88f811c7c7aa\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [\n",
      "      {\n",
      "        \"tenantId\": \"2f4a9838-26b7-47ee-be60-ccc1fdec5953\"\n",
      "      },\n",
      "      {\n",
      "        \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
      "      }\n",
      "    ],\n",
      "    \"name\": \"MCAPS-Hybrid-REQ-40165-2022-JakeWang\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"user\": {\n",
      "      \"name\": \"jacwang@microsoft.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"id\": \"997499f7-6523-407d-ac0c-d9ee154f1df1\",\n",
      "    \"isDefault\": false,\n",
      "    \"managedByTenants\": [\n",
      "      {\n",
      "        \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
      "      }\n",
      "    ],\n",
      "    \"name\": \"MCAPS-Hybrid-REQ-41592-2022-KamalAbburi\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"16b3c013-d300-468d-ac64-7eda0820b6d3\",\n",
      "    \"user\": {\n",
      "      \"name\": \"jacwang@microsoft.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Select the account you want to log in with. For more information on login with Azure CLI, see https://go.microsoft.com/fwlink/?linkid=2271136\n"
     ]
    }
   ],
   "source": [
    "!az login --tenant 16b3c013-d300-468d-ac64-7eda0820b6d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "Set the following variables for use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "azure_ai_connection_string = os.environ.get('AZURE_AI_PROJECT_URL')  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    "azure_openai_deployment = os.environ.get('AZURE_OPENAI_DEPLOYMENT')  # Your AOAI resource, you must use an AOAI GPT model\n",
    "azure_openai_api_version = os.environ.get('AZURE_OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional â€“ reuse an existing dataset\n",
    "dataset_name    = os.environ.get(\"DATASET_NAME\",    \"dataset-test\")\n",
    "dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to your Azure Open AI deployment\n",
    "To evaluate your LLM-generated data remotely in the cloud, we must connect to your Azure Open AI deployment. This deployment must be a GPT model which supports `chat completion`, such as `gpt-4`. To see the proper value for `conn_str`, navigate to the connection string at the \"Project Overview\" page for your Azure AI project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the project client (Foundry project and credentials)\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=azure_ai_connection_string,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use `evaluate_test_data.jsonl` which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the [`Simulator`](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/simulator-interaction-data) from Azure AI Evaluation SDK. \n",
    "\n",
    "Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your [registry](https://ml.azure.com/registries) or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nw_dataset  (version: 1.0, id: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/nw_dataset/versions/1.0)\n",
      "- eval-data  (version: 1.0, id: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/eval-data/versions/1.0)\n"
     ]
    }
   ],
   "source": [
    "# List each dataset with name, version and id\n",
    "for ds in project_client.datasets.list():\n",
    "    print(f\"- {ds.name}  (version: {ds.version}, id: {ds.id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'eval-data' with version '1.0' already exists.\n",
      "Using existing dataset with ID: azureai://accounts/ai-jacwang-1965/projects/evalproj/data/eval-data/versions/1.0\n"
     ]
    }
   ],
   "source": [
    "# Upload a local jsonl file (skip if you already have a Dataset registered)\n",
    "try:\n",
    "    data_id = project_client.datasets.upload_file(\n",
    "        name='eval-data',\n",
    "        version=dataset_version,\n",
    "        file_path=\"../../data/evaluate_test_data.jsonl\",\n",
    "    ).id\n",
    "    print(f\"Successfully uploaded dataset with ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    if \"409\" in str(e) or \"Conflict\" in str(e):\n",
    "        print(f\"Dataset 'eval-data' with version '{dataset_version}' already exists.\")\n",
    "        # Retrieve the existing dataset\n",
    "        datasets = project_client.datasets.list()\n",
    "        for ds in datasets:\n",
    "            if ds.name == 'eval-data' and ds.version == dataset_version:\n",
    "                data_id = ds.id\n",
    "                print(f\"Using existing dataset with ID: {data_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "The code below demonstrates how to configure the evaluators you want to run in AI Foundry.  Note, the EvaluatorConfiguration must be configured correctly for the entire cloud evaluation job to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": \"https://ai-jacwang-1965.cognitiveservices.azure.com/openai\", #os.environ.get(\"AZURE_AI_PROJECT_URL\"), # https://<account_name>.services.ai.azure.com\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'azure_endpoint': 'https://ai-jacwang-1965.cognitiveservices.azure.com/openai',\n",
       " 'api_key': '335nuZ4bUIaJgL6iyOjJN4Ggxk5jxX5v2GXGlYWHrMt11OZQWr04JQQJ99BEACHYHv6XJ3w3AAAAACOGYS8f',\n",
       " 'azure_deployment': 'gpt-4.1-mini'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import EvaluatorConfiguration, EvaluatorIds\n",
    "from azure.ai.evaluation import RougeType\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"response_completeness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RESPONSE_COMPLETENESS.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\"model_config\": model_config},\n",
    "    ),\n",
    "    \"hate_unfairness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.HATE_UNFAIRNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"content_safety\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CONTENT_SAFETY.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"self_harm\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SELF_HARM.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"sexual\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SEXUAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"protected_material\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.PROTECTED_MATERIAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value\n",
    "    ),\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.F1_SCORE.value\n",
    "    ),\n",
    "    \"gleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GLEU_SCORE.value\n",
    "    ),\n",
    "    \"meteor_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.METEOR_SCORE.value,\n",
    "        init_params={\"alpha\": 0.8},\n",
    "    ),\n",
    "    \"rouge_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.ROUGE_SCORE.value,\n",
    "        init_params={\"rouge_type\": RougeType.ROUGE_4},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"groundedness_pro\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS_PRO.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"code_vulnerability\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CODE_VULNERABILITY.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value\n",
    "    ),\n",
    "    \"indirect_attack\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INDIRECT_ATTACK.value\n",
    "    ),\n",
    "    \"retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RETRIEVAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"similarity\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SIMILARITY.value\n",
    "    ),\n",
    "    \"document_retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.DOCUMENT_RETRIEVAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"tool_call_accuracy\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TOOL_CALL_ACCURACY.value\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working evals, use to test in support region like East US2\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "azure_openai_deployment = os.environ.get('AZURE_OPENAI_DEPLOYMENT')  # Your AOAI resource, you must use an AOAI GPT model\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"hate_unfairness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.HATE_UNFAIRNESS.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GROUNDEDNESS.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value,\n",
    "    ),\n",
    "    \"code_vulnerability\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.CODE_VULNERABILITY.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.F1_SCORE.value,\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value,\n",
    "          init_params={\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"gleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.GLEU_SCORE.value,\n",
    "    ),\n",
    "    \"indirect_attack\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INDIRECT_ATTACK.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"meteor_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.METEOR_SCORE.value,\n",
    "        init_params={\"alpha\": 0.8},\n",
    "    ),\n",
    "    \"protected_material\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.PROTECTED_MATERIAL.value,\n",
    "        init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "            },\n",
    "    ),\n",
    "    \"retrieval\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RETRIEVAL.value,\n",
    "         init_params={\n",
    "                \"deployment_name\": azure_openai_deployment,\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            },\n",
    "    ),\n",
    "    \"rouge_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.ROUGE_SCORE.value,\n",
    "        init_params={\"rouge_type\": RougeType.ROUGE_4},\n",
    "    ),\n",
    "    \"self_harm\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SELF_HARM.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"sexual\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.SEXUAL.value,\n",
    "        init_params={\"azure_ai_project\": azure_ai_connection_string},\n",
    "    ),\n",
    "    \"text_similarity\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TEXT_SIMILARITY_GRADER.value,\n",
    "        init_params={\n",
    "            \"evaluation_metric\": \"fuzzy_match\",\n",
    "            \"input\": \"{{item.response}}\",\n",
    "            \"name\": \"similarity\",\n",
    "            \"pass_threshold\": 1,\n",
    "            \"reference\": \"{{item.ground_truth}}\",\n",
    "            \"deployment_name\": azure_openai_deployment,\n",
    "        },\n",
    "        data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            },\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cloud evaluation\n",
    "Below we demonstrate how to trigger a single-instance Cloud Evaluation remotely on a test dataset. This can be used for pre-deployment testing of your AI application. \n",
    " \n",
    "Here we pass in the `data_id` we would like to use for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AzureCliCredential.get_token_info failed: Failed to invoke the Azure CLI\n"
     ]
    },
    {
     "ename": "CredentialUnavailableError",
     "evalue": "Failed to invoke the Azure CLI",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutExpired\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_credentials\\azure_cli.py:234\u001b[0m, in \u001b[0;36m_run_command\u001b[1;34m(command, timeout)\u001b[0m\n\u001b[0;32m    226\u001b[0m     kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m\"\u001b[39m: subprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdin\u001b[39m\u001b[38;5;124m\"\u001b[39m: subprocess\u001b[38;5;241m.\u001b[39mDEVNULL,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron, AZURE_CORE_NO_COLOR\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    233\u001b[0m     }\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# non-zero return from shell\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Fallback check in case the executable is not found while executing subprocess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\subprocess.py:468\u001b[0m, in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\subprocess.py:552\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\subprocess.py:1211\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\subprocess.py:1632\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutExpired\u001b[0m: Command '['cmd', '/c', 'az account get-access-token --output json --resource https://ai.azure.com']' timed out after 10 seconds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mCredentialUnavailableError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m Evaluation(\n\u001b[0;32m      8\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCloud evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation of dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     data\u001b[38;5;241m=\u001b[39mInputDataset(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mdata_id),\n\u001b[0;32m     11\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39mevaluators,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run the evaluation \u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m evaluation_response \u001b[38;5;241m=\u001b[39m \u001b[43mproject_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi-key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation_response\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation_response\u001b[38;5;241m.\u001b[39mstatus)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\ai\\projects\\_validation.py:46\u001b[0m, in \u001b[0;36mapi_version_validation.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsupported:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     39\u001b[0m             [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m         )\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\ai\\projects\\operations\\_operations.py:1138\u001b[0m, in \u001b[0;36mEvaluationsOperations.create\u001b[1;34m(self, evaluation, **kwargs)\u001b[0m\n\u001b[0;32m   1135\u001b[0m _request\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mformat_url(_request\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_format_arguments)\n\u001b[0;32m   1137\u001b[0m _stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1138\u001b[0m pipeline_response: PipelineResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1142\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m201\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:240\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[0;32m    239\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 96 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_redirect.py:204\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    202\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[1;32m--> 204\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_retry.py:551\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[0;32m    550\u001b[0m request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 551\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[0;32m    553\u001b[0m     retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:157\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    :param request: The pipeline request object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:132\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.on_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_https(request)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_new_token:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m bearer_token \u001b[38;5;241m=\u001b[39m cast(Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessToken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessTokenInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token)\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_headers(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders, bearer_token)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:108\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._request_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mscopes: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Request a new token from the credential.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    This will call the credential's appropriate method to get a token and store it in the policy.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param str scopes: The type of access needed.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_authentication.py:98\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._get_token\u001b[1;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m TokenRequestOptions\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m:  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[0;32m     96\u001b[0m             options[key] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(key)  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSupportsTokenInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_credential\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(TokenCredential, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential)\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_credentials\\default.py:248\u001b[0m, in \u001b[0;36mDefaultAzureCredential.get_token_info\u001b[1;34m(self, options, *scopes)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Request an access token for `scopes`.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03mThis is an alternative to `get_token` to enable certain scenarios that require additional properties\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m   `message` attribute listing each authentication attempt and its error message.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_successful_credential:\n\u001b[1;32m--> 248\u001b[0m     token_info \u001b[38;5;241m=\u001b[39m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSupportsTokenInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_successful_credential\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m acquired a token from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_successful_credential\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_info\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_internal\\decorators.py:23\u001b[0m, in \u001b[0;36mlog_get_token.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m     25\u001b[0m             logging\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;28;01mif\u001b[39;00m within_credential_chain\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01melse\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m     26\u001b[0m         )\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_credentials\\azure_cli.py:125\u001b[0m, in \u001b[0;36mAzureCliCredential.get_token_info\u001b[1;34m(self, options, *scopes)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@log_get_token\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_token_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mscopes: \u001b[38;5;28mstr\u001b[39m, options: Optional[TokenRequestOptions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AccessTokenInfo:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Request an access token for `scopes`.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    This is an alternative to `get_token` to enable certain scenarios that require additional properties\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m      receive an access token.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_token_base\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_credentials\\azure_cli.py:147\u001b[0m, in \u001b[0;36mAzureCliCredential._get_token_base\u001b[1;34m(self, options, *scopes, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tenant:\n\u001b[0;32m    146\u001b[0m     command \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m --tenant \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tenant\n\u001b[1;32m--> 147\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_run_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m token \u001b[38;5;241m=\u001b[39m parse_token(output)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\site-packages\\azure\\identity\\_credentials\\azure_cli.py:258\u001b[0m, in \u001b[0;36m_run_command\u001b[1;34m(command, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# could be a timeout, for example\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     error \u001b[38;5;241m=\u001b[39m CredentialUnavailableError(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to invoke the Azure CLI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mCredentialUnavailableError\u001b[0m: Failed to invoke the Azure CLI"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "\n",
    "# Create an evaluation with the dataset and evaluators specified\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=InputDataset(id=data_id),\n",
    "    evaluators=evaluators,\n",
    ")\n",
    "\n",
    "# Run the evaluation \n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"api-key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Created evaluation:\", evaluation_response.name)\n",
    "print(\"Status:\", evaluation_response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation for: relevance\n",
      "Created evaluation: 6f2b7a34-59b9-40a8-a6bb-603070ecb453\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: hate_unfairness\n",
      "Created evaluation: 6f2b7a34-59b9-40a8-a6bb-603070ecb453\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: hate_unfairness\n",
      "Created evaluation: 837bd6fe-9e84-4693-b159-e62bc076aa8e\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: violence\n",
      "Created evaluation: 837bd6fe-9e84-4693-b159-e62bc076aa8e\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: violence\n",
      "Created evaluation: 615371da-9b0e-4e20-8830-b695cadecc3d\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: groundedness\n",
      "Created evaluation: 615371da-9b0e-4e20-8830-b695cadecc3d\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: groundedness\n",
      "Created evaluation: 3fc674b4-499a-4547-ad26-ad24e19b4535\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: bleu_score\n",
      "Created evaluation: 3fc674b4-499a-4547-ad26-ad24e19b4535\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: bleu_score\n",
      "Created evaluation: 13cdf27b-bc44-4036-98cc-655adee24e63\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: coherence\n",
      "Created evaluation: 13cdf27b-bc44-4036-98cc-655adee24e63\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: coherence\n",
      "Created evaluation: d88baa09-3160-4b12-8c27-ae02e7eaa510\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: f1_score\n",
      "Created evaluation: d88baa09-3160-4b12-8c27-ae02e7eaa510\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: f1_score\n",
      "Created evaluation: 47c24e13-3c35-409d-ae29-2109eeb23727\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: fluency\n",
      "Created evaluation: 47c24e13-3c35-409d-ae29-2109eeb23727\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: fluency\n",
      "Created evaluation: f41ad411-d59a-4968-9f4a-b657d74fc83a\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: gleu_score\n",
      "Created evaluation: f41ad411-d59a-4968-9f4a-b657d74fc83a\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: gleu_score\n",
      "Created evaluation: ca82ddd4-757d-4642-8f7a-7ffa5a9dd88d\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: intent_resolution\n",
      "Created evaluation: ca82ddd4-757d-4642-8f7a-7ffa5a9dd88d\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: intent_resolution\n",
      "Created evaluation: c88bf933-2b00-4bb3-be02-40b18a504226\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: meteor_score\n",
      "Created evaluation: c88bf933-2b00-4bb3-be02-40b18a504226\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: meteor_score\n",
      "Created evaluation: 748eb914-becc-4993-badc-53889bcc6205\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: protected_material\n",
      "Created evaluation: 748eb914-becc-4993-badc-53889bcc6205\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: protected_material\n",
      "Created evaluation: 353d3b14-61a3-4cc7-8a3b-1bef36290987\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: retrieval\n",
      "Created evaluation: 353d3b14-61a3-4cc7-8a3b-1bef36290987\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: retrieval\n",
      "Created evaluation: e555e8cf-1db7-409b-a448-52918baed9e5\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: rouge_score\n",
      "Created evaluation: e555e8cf-1db7-409b-a448-52918baed9e5\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: rouge_score\n",
      "Created evaluation: 0c09c368-0142-441d-ba56-1136039450c8\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: self_harm\n",
      "Created evaluation: 0c09c368-0142-441d-ba56-1136039450c8\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: self_harm\n",
      "Created evaluation: 00443ce3-a196-4e22-a620-c81e99cda06a\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: sexual\n",
      "Created evaluation: 00443ce3-a196-4e22-a620-c81e99cda06a\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: sexual\n",
      "Created evaluation: 145358e0-ca53-4ef2-a446-7f1a59e1530b\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: text_similarity\n",
      "Created evaluation: 145358e0-ca53-4ef2-a446-7f1a59e1530b\n",
      "Status: NotStarted\n",
      "\n",
      "Running evaluation for: text_similarity\n",
      "Created evaluation: cf77a792-8e40-4f0b-8845-ff4f70071bcc\n",
      "Status: NotStarted\n",
      "Created evaluation: cf77a792-8e40-4f0b-8845-ff4f70071bcc\n",
      "Status: NotStarted\n"
     ]
    }
   ],
   "source": [
    "# Test each evaluator individually using the same dataset\n",
    "from azure.ai.projects.models import Evaluation, InputDataset\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "for eval_name, eval_config in evaluators.items():\n",
    "    print(f\"\\nRunning evaluation for: {eval_name}\")\n",
    "    single_eval = Evaluation(\n",
    "        display_name=f\"Cloud evaluation - {eval_name}\",\n",
    "        description=f\"Evaluation of dataset with {eval_name}\",\n",
    "        data=InputDataset(id=data_id),\n",
    "        evaluators={eval_name: eval_config},\n",
    "    )\n",
    "    try:\n",
    "        eval_response = project_client.evaluations.create(\n",
    "            single_eval,\n",
    "            headers={\n",
    "                \"model-endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                \"api-key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "            },\n",
    "        )\n",
    "        print(f\"Created evaluation: {eval_response.name}\")\n",
    "        print(f\"Status: {eval_response.status}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running evaluator {eval_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Evaluation status:  NotStarted\n",
      "AI Foundry Portal URI:  https://ai.azure.com/resource/build/evaluation/71c2f535-8088-46f8-a0c9-a3324b72a50f?wsid=/subscriptions/6025ba02-1dfd-407f-b358-88f811c7c7aa/resourceGroups/aigent_eus/providers/Microsoft.CognitiveServices/accounts/ai-jacwang-1965/projects/evalproj&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = evaluation_response\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "#print(\"Created evaluation, evaluation ID: \", get_evaluation_response.data.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and download evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from azure.ai.projects.models import Evaluation\n",
    "\n",
    "# Poll for evaluation completion\n",
    "eval_name = evaluation_response.name\n",
    "while True:\n",
    "    ev = project_client.evaluations.get(eval_name)\n",
    "    status = ev.status\n",
    "    print(f\"Evaluation status: {status}\")\n",
    "    if status in ['Completed', 'Failed', 'Canceled']:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "# List evaluation result versions for this evaluation\n",
    "print('Evaluation result versions:')\n",
    "for res in project_client.evaluation_results.list_versions(eval_name):\n",
    "    print(f\"- version: {res.version}, blob URI: {res.blob_uri}\")\n",
    "    result = res\n",
    "    break\n",
    "\n",
    "# Download the instance results JSONL\n",
    "import requests\n",
    "blob_url = result.blob_uri\n",
    "r = requests.get(blob_url)\n",
    "with open('instance_results.jsonl', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "print('Downloaded instance_results.jsonl')\n",
    "\n",
    "# Preview the first few rows\n",
    "with open('instance_results.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
